{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "278f2bG1utUn"
      },
      "source": [
        "**STEPS**\n",
        "1. Data select via SQL\n",
        "2. Data cleaning\n",
        "3. Converting categorical vector into binary via OneHotEncoding (OHE)\n",
        "4. Drop out useless columns after OHE: housing_no', 'loan_no', 'client_id', 'education_unknown', 'poutcome_other','poutcome_failure', 'y_no', 'job_unknown'\n",
        "5. Data standardization and normalization via SMOTE\n",
        "6. Train predictive models: Logistic Regression, SVM, Neuron Networks, Random Forest\n",
        "7. Create 2 data client group with different logic and compare its ratio between probability and estimated conversion value\n",
        "8. Calculate of estimated conversion rate and value for final selected top 5000 clients \n",
        "\n",
        "**OVERVIEW**\n",
        "\n",
        "Our commitment to data-driven marketing has yielded valuable insights to optimize our campaign. We employed advanced techniques like one-hot encoding, SMOTE, and A/B testing to refine our targeting strategy and best data model.\n",
        "\n",
        "This analysis revealed a critical finding: prioritizing candidates with a high probability of conversion (Group 2) holds significant promise. Compared to the initial focus on high expected revenue alone (Group 1), Group 2 demonstrates best ratio between probability and estimated conversion value.\n",
        "\n",
        "These data-driven results make us confident that the campaign will achieve successful outcomes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xOg09JZ79Ot"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect('/content/data.db')\n",
        "\n",
        "# Data retrieve from given dataset into DataFrame\n",
        "dt_select = '''\n",
        "SELECT DISTINCT * from Accounts a\n",
        "LEFT JOIN Campaigns c on a.id = c.account_id\n",
        "LEFT JOIN Clients cl on a.client_id = cl.id\n",
        "LEFT JOIN Outcomes o on o.campaign_id = c.id\n",
        "WHERE a.in_default = 'no' AND c.pdays != -1 AND poutcome != 'unknown'\n",
        "'''\n",
        "\n",
        "df = pd.read_sql_query(dt_select, conn)\n",
        "\n",
        "# Close connection\n",
        "conn.close()\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pj6vSGcoiPp"
      },
      "outputs": [],
      "source": [
        "# We dropped the columns that we do not find as important\n",
        "df.drop(columns=(['account_id', 'campaign_id','in_default', 'contact', 'id', 'day', 'month', 'duration', 'previous', 'campaign', 'pdays']), inplace=True, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI_1cI-GokFY"
      },
      "outputs": [],
      "source": [
        "# Saving df into csv file\n",
        "df.to_csv('1_phase_5.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVTSP8R4cBh"
      },
      "source": [
        "Converting categorical vector into binary via OneHotEncoding (OHE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtKVKAmSomIx"
      },
      "outputs": [],
      "source": [
        "# One hot encoding\n",
        "columns_to_encode = ['housing', 'loan', 'job', 'marital', 'education', 'poutcome', 'y']\n",
        "\n",
        "df_encoded = pd.get_dummies(df, columns=columns_to_encode)\n",
        "#df_encoded.tail()\n",
        "\n",
        "boolean_columns = df_encoded.select_dtypes(include=['bool']).columns\n",
        "df_encoded[boolean_columns] = df_encoded[boolean_columns].astype(int)\n",
        "\n",
        "df_encoded.head()\n",
        "df_encoded.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlp2Xks9ooB6"
      },
      "outputs": [],
      "source": [
        "df_encoded.head()\n",
        "# saving the OHE data\n",
        "df_encoded.to_csv('1_phase_df_encoded.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGKz5e7Qoyt9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a dataframe\n",
        "df = pd.read_csv('1_phase_df_encoded.csv')\n",
        "\n",
        "# Display the dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RWroBbIo1c2"
      },
      "outputs": [],
      "source": [
        "df = df[~(df['poutcome_other'] == 1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop out useless columns after OHE: housing_no', 'loan_no', 'client_id', 'education_unknown', 'poutcome_other','poutcome_failure', 'y_no', 'job_unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWph8VvJo3gB"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = ['housing_no', 'loan_no', 'client_id', 'education_unknown', 'poutcome_other','poutcome_failure', 'y_no', 'job_unknown']\n",
        "df = df.drop(columns=columns_to_drop)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsqGV1Nn4ggV"
      },
      "source": [
        "Data standardization and normalization via SMOTE method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QTBS_jJo7CT"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "# Rename dataframe to match user's dataframe name\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop(columns=['y_yes'])\n",
        "y = df['y_yes']\n",
        "\n",
        "# Apply SMOTE for over-sampling\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Combine resampled features and target variable\n",
        "df_resampled = pd.concat([X_res, y_res], axis=1)\n",
        "\n",
        "# Check the distribution of the target variable to ensure balance\n",
        "resampled_distribution = df_resampled['y_yes'].value_counts()\n",
        "print(resampled_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q_QuYyRo9H6"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcectGFwo_Fe"
      },
      "outputs": [],
      "source": [
        "df_resampled.to_csv('df_SMOTE.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnU1lyKWrWXR"
      },
      "source": [
        "TRAIN MODELS:\n",
        "1. Logistic Regression\n",
        "2. SVM\n",
        "3. Neuron Networks\n",
        "4. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNQ2UXvgrt-v"
      },
      "source": [
        "1. Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74rMJ7Ob-Zi2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load Data and Define Target Variable\n",
        "data = pd.read_csv('df_SMOTE2.csv')\n",
        "\n",
        "target = 'y_yes'  # Assuming 'y_yes' is the target variable\n",
        "\n",
        "# Step 2: Preprocess Data\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum().sum()\n",
        "print(\"Missing values:\", missing_values)\n",
        "\n",
        "# Handle missing values if there are any (e.g., fillna or dropna)\n",
        "# data = data.fillna(method='ffill')  # Example of filling missing values\n",
        "# Separate features and target\n",
        "X = data.drop(target, axis=1)\n",
        "y = data[target]\n",
        "\n",
        "# Ensure target variable is suitable for classification (optional)\n",
        "print(\"Target variable data type:\", y.dtype)\n",
        "\n",
        "# Standardize data (recommended for weighted classifiers)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.6, random_state=42)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Step 3: Train and Evaluate Logistic Regression Model\n",
        "\n",
        "# Define and train the Logistic Regression model\n",
        "model = LogisticRegression(class_weight=class_weights_dict)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Logistic Regression - Accuracy:\", accuracy)\n",
        "print(\"Logistic Regression - Precision:\", precision)\n",
        "print(\"Logistic Regression - Recall:\", recall)\n",
        "print(\"Logistic Regression - F1-score:\", f1)\n",
        "\n",
        "##############################################################\n",
        "\n",
        "# Step 4: Identify Top 5000 Likely Buyers\n",
        "\n",
        "# Predict probabilities for the entire dataset using Logistic Regression\n",
        "probabilities = model.predict_proba(X_scaled)[:, 1]\n",
        "\n",
        "# Create a DataFrame with the original data and the predicted probabilities\n",
        "data_with_prob = data.copy()\n",
        "data_with_prob['probability'] = probabilities\n",
        "\n",
        "# Sort the DataFrame by probability in descending order\n",
        "data_with_prob_sorted = data_with_prob.sort_values(by='probability', ascending=False)\n",
        "\n",
        "# Select the top 5000 individuals\n",
        "top_5000 = data_with_prob_sorted.head(5000)\n",
        "\n",
        "# Display the top 5000 individuals\n",
        "print(top_5000)\n",
        "\n",
        "# Optionally, save the top 5000 individuals to a CSV file\n",
        "top_5000.to_csv('top_5000_customers.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgkh15znrxx5"
      },
      "source": [
        "2. SVM with using GridSearchCV with 5- fold and 10-fold cross-validation in pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBfAxXwwrxmY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/df_SMOTE2.csv')\n",
        "\n",
        "#  Separate features and target\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Pipeline for model training\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(probability=True))  # Setting probability=True\n",
        "])\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
        "    'svm__C': [0.1, 1, 10, 100],\n",
        "    'svm__gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# GridSearchCV with 10-fold cross-validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Prediction and evaluation of model\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Compute and display the confusion matrix\n",
        "con_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(con_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Probabality prediction for whole dataset\n",
        "scaler = grid_search.best_estimator_.named_steps['scaler']\n",
        "svm_model = grid_search.best_estimator_.named_steps['svm']\n",
        "y_prob_full = svm_model.predict_proba(scaler.transform(X))\n",
        "\n",
        "print(\"Predicted probabilities for the entire dataset:\\n\", y_prob_full)\n",
        "\n",
        "# Evaluation- ROC AUC score\n",
        "y_prob = svm_model.predict_proba(scaler.transform(X_test))\n",
        "roc_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "print(\"ROC AUC Score:\", roc_auc)\n",
        "\n",
        "# Evaluation- ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1])\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Prediction of new data\n",
        "new_data = pd.read_csv('/content/to_clean_cleaned.csv')\n",
        "new_data_drop = new_data.drop(columns=['client_id', 'y_yes'])\n",
        "\n",
        "\n",
        "# Probabality prediction for new data\n",
        "new_predictions = grid_search.best_estimator_.predict_proba(new_data_drop)\n",
        "print(\"New Predictions Probabilities:\\n\", new_predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MRzgvPtsUkY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loaded dataset\n",
        "data = pd.read_csv('/content/df_SMOTE2.csv')\n",
        "\n",
        "# Data are devided based on the last column: 'y_yes'\n",
        "X = data.iloc[:, :-1]  # all columns except 'y_yes' column\n",
        "y = data.iloc[:, -1]   # 'y_yes' column\n",
        "\n",
        "# Setting data into the training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid for GrindSearchCV\n",
        "param_grid = {\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(SVC(probability=True), param_grid, cv=10, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best parameters found:\", best_params)\n",
        "\n",
        "# Prediction and evaluation of model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Neuron Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agxb2SdqGrrs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Loaded dataset\n",
        "data = pd.read_csv('/content/df_SMOTE2.csv')\n",
        "\n",
        "# Data are devided based on the last column: 'y_yes'\n",
        "X = data.iloc[:, :-1]  # all columns except 'y_yes' column\n",
        "y = data.iloc[:, -1]   # 'y_yes' column\n",
        "\n",
        "# Setting data into the training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=42)\n",
        "\n",
        "perceptron = Perceptron(max_iter=5, random_state=42)\n",
        "\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "y_pred = perceptron.predict(X_test)\n",
        "print(y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWz9H_tKuDlj"
      },
      "source": [
        "4. Random Forest with using GridSearchCV with 5-fold cross-validation and KFold with 5-fold cross-validation in pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BEuzAppuFxB"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_predict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('df_SMOTE2.csv')\n",
        "X = data.iloc[:, :-1] # Features\n",
        "y= data.iloc[:, -1] # Target variable\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest classifier with 25 trees\n",
        "clf_25 = RandomForestClassifier(n_estimators=25, criterion='log_loss', min_samples_leaf=4, random_state=42)\n",
        "clf_25.fit(X_train, y_train)\n",
        "y_pred_25 = clf_25.predict(X_test)\n",
        "accuracy_25 = accuracy_score(y_test, y_pred_25)\n",
        "print(\"Classification Report:\\n\", classification_report(y, y_pred_25))\n",
        "print(\"Accuracy with 25 trees: {:.2f}%\".format(accuracy_25 * 100))\n",
        "\n",
        "# Random Forest classifier with 50 trees\n",
        "clf_50 = RandomForestClassifier(n_estimators=50, criterion='log_loss', min_samples_leaf=4, random_state=42)\n",
        "clf_50.fit(X_train, y_train)\n",
        "y_pred_50 = clf_50.predict(X_test)\n",
        "accuracy_50 = accuracy_score(y_test, y_pred_50)\n",
        "print(\"Classification Report:\\n\", classification_report(y, y_pred_50))\n",
        "print(\"Accuracy with 50 trees: {:.2f}%\".format(accuracy_50 * 100))\n",
        "\n",
        "# Random Forest classifier with 75 trees\n",
        "clf_75 = RandomForestClassifier(n_estimators=75, criterion='log_loss', min_samples_leaf=4, random_state=42)\n",
        "clf_75.fit(X_train, y_train)\n",
        "y_pred_75 = clf_75.predict(X_test)\n",
        "accuracy_75 = accuracy_score(y_test, y_pred_75)\n",
        "print(\"Classification Report:\\n\", classification_report(y, y_pred_75))\n",
        "print(\"Accuracy with 75 trees: {:.2f}%\".format(accuracy_75 * 100))\n",
        "\n",
        "# Create a Random Forest classifier with 200 trees\n",
        "clf_200 = RandomForestClassifier(n_estimators=200, criterion='log_loss', min_samples_leaf=4, random_state=42)\n",
        "clf_200.fit(X_train, y_train)\n",
        "y_pred_200 = clf_200.predict(X_test)\n",
        "accuracy_200 = accuracy_score(y_test, y_pred_200)\n",
        "print(\"Classification Report:\\n\", classification_report(y, y_pred_200))\n",
        "print(\"Accuracy with 200 trees: {:.2f}%\".format(accuracy_200 * 100))\n",
        "\n",
        "# Compute and display the confusion matrix of winning Random Forest with 200 trees\n",
        "cm = confusion_matrix(y, y_pred_200)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "disp.plot(cmap='Blues')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using Stratified K-Fold cross-validation for estimating predictive performance of Random Forest model with 200 trees (unbalanced dataset)\n",
        "# Initialize Stratified K-Fold with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and get predictions\n",
        "y_pred_200 = cross_val_predict(clf_200, X, y, cv=skf)\n",
        "\n",
        "# Compute and print the accuracy\n",
        "accuracy_skf = accuracy_score(y, y_pred)\n",
        "print(\"Overall Accurancy: {:.2f}%\".format(accuracy_skf * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAIZABbeuG8h"
      },
      "outputs": [],
      "source": [
        "# GridSearchCV with 5-fold cross-validation\n",
        "pipe_rf = Pipeline([\n",
        "    ('pca', PCA(n_components=2)),\n",
        "    ('clf', GridSearchCV(\n",
        "        estimator=RandomForestClassifier(random_state=42),\n",
        "        param_grid={\n",
        "            \"n_estimators\": [75, 50, 100,200],\n",
        "            \"min_samples_split\": [2, 3, 4, 5],\n",
        "            \"min_samples_leaf\": [1,2,3]},\n",
        "        cv=5, verbose=0)\n",
        "    )\n",
        "])\n",
        "pipe_rf.fit(X_train,y_train)\n",
        "y_pred = pipe_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy after PCA and GridSearchCV: {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the new dataset with predictions\n",
        "output_file_path = 'new_data_with_predictions.csv'\n",
        "new_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Predictions have been added to the new dataset and saved to {output_file_path}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3jaB3o7JdAD"
      },
      "source": [
        "Select top 5000 clients with high perfomance of estimated conversion value\n",
        "\n",
        "Create 2 data client groups and compare its ratio between probability and estimated conversion value\n",
        "\n",
        "**Group 1** - Sorted by expected_revenue descending with probability >= 0,3\n",
        "\n",
        "**Group 2** - Sorted by probability only descending, demonstrates best ratio between probability and estimated conversion value => final select by this method\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnljsCmbpQrO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import shutil\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'Sorted_Client_Data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop dupliccates by client_id\n",
        "df = df.drop_duplicates(subset='client_id')\n",
        "\n",
        "# Convert expected_revenue into numeric datatype\n",
        "df['expected_revenue'] = pd.to_numeric(df['expected_revenue'], errors='coerce')\n",
        "\n",
        "# Filtering clients who are not 'in_default'\n",
        "df = df[df['in_default'] == 'no']\n",
        "\n",
        "# Create group1\n",
        "group1 = df[df['prob_1'] >= 0.3].sort_values(by='expected_revenue', ascending=False)\n",
        "\n",
        "# Create group2\n",
        "group2 = df.sort_values(by='prob_1', ascending=False)\n",
        "\n",
        "\n",
        "# Create ratio for comparison between group1 and group2\n",
        "total_clients = 5000\n",
        "results = []\n",
        "\n",
        "for g1_ratio in range(11):\n",
        "    g2_ratio = 10 - g1_ratio\n",
        "    n_g1 = (g1_ratio * total_clients) // 10\n",
        "    n_g2 = total_clients - n_g1\n",
        "\n",
        "    selected_group1 = group1.head(n_g1)\n",
        "    selected_group2 = group2.head(n_g2)\n",
        "\n",
        "    combined_group = pd.concat([selected_group1, selected_group2]).drop_duplicates(subset='client_id').head(total_clients)\n",
        "\n",
        "    avg_prob_1 = combined_group['prob_1'].mean()\n",
        "    total_expected_revenue = combined_group['expected_revenue'].sum()\n",
        "\n",
        "    results.append({\n",
        "        'group1_ratio': g1_ratio / 10,\n",
        "        'group2_ratio': g2_ratio / 10,\n",
        "        'avg_prob_1': avg_prob_1,\n",
        "        'total_expected_revenue': total_expected_revenue\n",
        "    })\n",
        "\n",
        "# Convert into Dataframe for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df)\n",
        "\n",
        "# Select top 5000 clients by prob_1 descending\n",
        "top_5000_clients = df.nlargest(5000, 'prob_1')\n",
        "\n",
        "# Calculate average probability and total expected revenue\n",
        "average_prob_1 = top_5000_clients['prob_1'].mean()\n",
        "total_expected_revenue = top_5000_clients['expected_revenue'].sum()\n",
        "\n",
        "print(f'Average prob_1: {average_prob_1}')\n",
        "print(f'Total expected revenue: {total_expected_revenue}')\n",
        "\n",
        "# Save top 5000 clients into csv file and create dowload function\n",
        "output_file_path = 'Top_5000_Clients.csv'\n",
        "top_5000_clients.to_csv(output_file_path, index=False)\n",
        "\n",
        "def download_file(file_path, output_path):\n",
        "    shutil.copy(file_path, output_path)\n",
        "    print(f'Soubor byl ulo≈æen na: {output_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO98zhQSKLoU"
      },
      "source": [
        "Calculate of estimated conversion rate and value for final select top 5000 clients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnff0bULKMo6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('Top_5000_Clients.csv')\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(data.head())\n",
        "\n",
        "def calculate_estimated_conversion(df):\n",
        "\n",
        "    # Calculate the estimated conversion rate\n",
        "    estimated_conversion_rate = df['prob_1'].mean()\n",
        "\n",
        "    # Calculate the estimated conversion value\n",
        "    estimated_conversion_value = (df['prob_1'] * df['monthly_income']).sum()\n",
        "\n",
        "    return estimated_conversion_rate, estimated_conversion_value\n",
        "\n",
        "# Perform the calculation\n",
        "estimated_conversion_rate, estimated_conversion_value = calculate_estimated_conversion(data)\n",
        "\n",
        "print(f\"Estimated Conversion Rate: {estimated_conversion_rate * 100:.2f}%\")\n",
        "print(f\"Estimated Conversion Value: {estimated_conversion_value:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
